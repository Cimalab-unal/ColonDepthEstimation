{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1FLbbzwO-kNzZmAc6IBPKqb6FqXv284vD","timestamp":1599187539768}],"collapsed_sections":["-eRIDO--rnWP"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"gpuClass":"standard"},"cells":[{"cell_type":"code","metadata":{"id":"H01NIpq-qOjq"},"source":["import os, pathlib\n","!pip uninstall gdown -y && pip install gdown\n","if not os.path.exists('/content/weights'): os.mkdir('/content/weights')\n","if not os.path.exists('/content/data'): os.mkdir('/content/data')\n","if not os.path.exists('/content/results'): os.mkdir('/content/results')\n","if not os.path.exists('/content/weights/EfficientNetB0-CurriculumLearning.h5'):\n","  !gdown 1FLI2r7MtfczKdt9C4Og0mvDGtoD7Jq92 -O /content/weights/EfficientNetB0-CurriculumLearning.h5\n","if not os.path.exists('/content/SyntheticDatabase_testingset.zip'):\n","  !gdown 1vZgc6ofqKcRXVJUHCZ9Lb1r6SFCuuV51 -O /content/SyntheticDatabase_testingset.zip\n","  !unzip '/content/SyntheticDatabase_testingset.zip' -d '/content/data'\n","\n","import time, pathlib\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.layers import Layer, InputSpec\n","from tensorflow.python.keras.utils import conv_utils\n","from skimage.transform import resize\n","import numpy as np\n","import cv2\n","\n","%reload_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Utils"],"metadata":{"id":"-eRIDO--rnWP"}},{"cell_type":"code","source":["class BilinearUpSampling2D(Layer):\n","    def __init__(self, size=(2, 2), data_format=None, **kwargs):\n","        super(BilinearUpSampling2D, self).__init__(**kwargs)\n","        self.data_format = self.normalize_data_format(data_format)\n","        self.size = conv_utils.normalize_tuple(size, 2, 'size')\n","        self.input_spec = InputSpec(ndim=4)\n","\n","    def compute_output_shape(self, input_shape):\n","        if self.data_format == 'channels_first':\n","            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n","            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n","            return (input_shape[0],\n","                    input_shape[1],\n","                    height,\n","                    width)\n","        elif self.data_format == 'channels_last':\n","            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n","            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n","            return (input_shape[0],\n","                    height,\n","                    width,\n","                    input_shape[3])\n","\n","    def call(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        if self.data_format == 'channels_first':\n","            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n","            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n","        elif self.data_format == 'channels_last':\n","            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n","            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n","        return tf.compat.v1.image.resize(inputs, [height, width], method=tf.image.ResizeMethod.BILINEAR, align_corners=True)\n","\n","    def get_config(self):\n","        config = {'size': self.size, 'data_format': self.data_format}\n","        base_config = super(BilinearUpSampling2D, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def normalize_data_format(self, value):\n","        if value is None:\n","            value = tf.keras.backend.image_data_format()\n","        data_format = value.lower()\n","        if data_format not in {'channels_first', 'channels_last'}:\n","            raise ValueError('The `data_format` argument must be one of '\n","                         '\"channels_first\", \"channels_last\". Received: ' +\n","                         str(value))\n","        return data_format\n","\n","def DepthNorm(x, maxDepth):\n","    return maxDepth / x\n","\n","def resize_img(img, resolution=480):\n","    return resize(img, (resolution, int(resolution*4/3)), preserve_range=True, mode='reflect', anti_aliasing=True )\n","\n","def predict(model, images, minDepth=0.1, maxDepth=25, batch_size=2):\n","    # Support multiple RGBs, one RGB image, even grayscale\n","    if len(images.shape) < 3: images = np.stack((images,images,images), axis=2)\n","    if len(images.shape) < 4: images = images.reshape((1, images.shape[0], images.shape[1], images.shape[2]))\n","    # Compute predictions\n","    predictions = model.predict(images, batch_size=batch_size, verbose=0)\n","\n","    # Put in expected range\n","    return np.clip(DepthNorm(predictions, maxDepth=25), minDepth, maxDepth) / maxDepth\n","\n","def scale_up(scale, images):\n","    from skimage.transform import resize\n","    scaled = []\n","\n","    for i in range(len(images)):\n","        img = images[i]\n","        output_shape = (scale * img.shape[0], scale * img.shape[1])\n","        scaled.append( resize(img, output_shape, order=1, preserve_range=True, mode='reflect', anti_aliasing=True ) )\n","\n","    return np.stack(scaled)\n","\n","def gamma_table(gamma=1.0):\n","    invGamma = 1.0 / gamma\n","    table_encoded = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype('uint8')\n","    table_decoded = np.array([((i / 255.0) ** gamma) * 255 for i in np.arange(0, 256)]).astype('uint8')\n","    return table_encoded, table_decoded\n","\n","def compute_errors(gt, pred):\n","  thresh = np.maximum((gt / pred), (pred / gt))\n","  a1 = (thresh < 1.25   ).mean()\n","  a2 = (thresh < 1.25 ** 2).mean()\n","  a3 = (thresh < 1.25 ** 3).mean()\n","  abs_rel = np.mean(np.abs(gt - pred) / gt)\n","  rmse = (gt - pred) ** 2\n","  rmse = np.sqrt(rmse.mean())\n","  log_10 = (np.abs(np.log10(gt)-np.log10(pred))).mean()\n","  return a1, a2, a3, abs_rel, rmse, log_10\n","\n","\n","def sortingFiles(arr, n):\n","  arr.sort(key = lambda x: (len(x), x))\n","  return arr\n","\n","def findFirstVector(value,vector):\n","  res = next(x for x, val in enumerate(vector) if val > value)\n","  return res\n","\n","def evaluate_colon(model, batch_size=1, maxDepth=25, path_data = 'data_test/', params={}, verbose=True):\n","\n","    bs = batch_size\n","    model_path = params['model_path']\n","    path_save = params['path_save']\n","    isRealVid = params['isRealVid']\n","    isSingleFolder = params['isSingleFolder']\n","    imgSave = params['imgSave']\n","    data_set=params['data_set']\n","\n","    shape_rgb = (bs, 480, 640, 3)\n","    shape_depth = (bs, 480, 640)\n","    table_gamma, decode = gamma_table(gamma=1.5)\n","\n","    for vid_name in os.listdir(path_data):\n","\n","        if isSingleFolder:\n","          vid_path = path_data\n","          vid_path_save = path_save\n","        else:\n","          vid_path = path_data + vid_name + '/'\n","          vid_path_save = path_save + vid_name + '/'\n","        if not os.path.isdir(vid_path): continue\n","\n","        print('\\n------------------------------------------------------------------------------')\n","        print(vid_path)\n","\n","\n","        if not isSingleFolder and not(os.path.exists(path_save + vid_name + '/')):\n","            os.mkdir(path_save + vid_name + '/')\n","\n","\n","        file_list = []\n","        for file in os.listdir(vid_path+'img/'):\n","            file_list.append(file)\n","\n","\n","        files = sortingFiles(file_list,len(file_list))\n","        nfr = len(files)\n","\n","        depth_scores = np.zeros((6, nfr))\n","\n","\n","        for i in range(nfr//bs):\n","\n","            x = np.ndarray(shape_rgb)\n","            true_y = np.ndarray(shape_depth)\n","            idx = 0\n","            for ii in range(i*bs,(i+1)*bs,1):\n","\n","                img= cv2.imread(vid_path + 'img/' + files[ii])\n","\n","                x[idx,:,:,:] = resize_img(img,480)\n","\n","                if not isRealVid:\n","                    true_y[idx,:,:] = resize_img(cv2.imread(vid_path + 'z/' + files[ii])[:,:,0],480)\n","\n","\n","                true_y[idx,:,:] = np.clip(np.asarray(true_y[idx,:,:], dtype=float) / 255, 0, 1)\n","                idx += 1\n","\n","            # Compute results\n","            pred_y = scale_up(2, predict(model, x/255, minDepth=0.1,\n","                                         maxDepth=25, batch_size=bs)[:,:,:,0])\n","\n","            idx = 0\n","            y_gamma = np.zeros((bs,480,640),dtype=np.uint8)\n","\n","            # Invert gamma correction\n","            for ii in range(i*bs,(i+1)*bs,1):\n","\n","                y_gamma[idx,:,:] = cv2.LUT((pred_y[idx,:,:]*255).astype('uint8'), decode)\n","                pred_color = np.uint8(y_gamma[idx,:,:])\n","\n","\n","                if imgSave:\n","\n","                  vis = pred_color\n","                  print('save: ' + vid_path_save + files[ii])\n","                  cv2.imwrite(vid_path_save + files[ii], vis)\n","\n","                idx += 1\n","\n","            # Compute errors per image in batch\n","            if not isRealVid:\n","                for j in range(len(true_y)):\n","                    errors = compute_errors(true_y[j]*maxDepth, (y_gamma[j]/255)*maxDepth)\n","\n","\n","                for k in range(len(errors)):\n","                    depth_scores[k][(i*bs)+j] = errors[k]\n","\n","                if imgSave:\n","                  print(vid_name + ' batch '+ str(i)+'/'+str(nfr//bs) + \\\n","                        ' rmse: ' + str(round(depth_scores[4][(i*bs)+j],3)) + \\\n","                        ' acc: ' + str(round(depth_scores[0][(i*bs)+j],3)))\n","                else:\n","                  print('\\r\\t'+vid_name + ' batch '+ str(i)+'/'+str(nfr//bs) + \\\n","                        ' rmse: ' + str(round(depth_scores[4][(i*bs)+j],3)) + \\\n","                        ' acc: ' + str(round(depth_scores[0][(i*bs)+j],3)), end='')\n","            else:\n","                print(vid_name + ' batch '+ str(i)+'/'+str(nfr//bs))\n","\n","\n","\n","        e = depth_scores.mean(axis=1)\n","        if not isRealVid:\n","            np.savetxt(vid_path + 'error.csv', e, delimiter='')\n","            if verbose:\n","                print('\\n-----------------------')\n","                print('Metrics of', vid_name)\n","                print(\"{:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}\".format( \\\n","                        'a1', 'a2', 'a3', 'rel', 'rms', 'log_10'))\n","                print(\"{:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}\".format(e[0],e[1],e[2],e[3],e[4],e[5]))\n","\n","        if isSingleFolder == True: break\n","    return e"],"metadata":{"id":"zwLL4t9frnDS","executionInfo":{"status":"ok","timestamp":1710113387107,"user_tz":-60,"elapsed":11,"user":{"displayName":"Josué Ruano Balseca","userId":"01224331386039879086"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Main"],"metadata":{"id":"5HT1_1Ghrize"}},{"cell_type":"code","source":["params = {}\n","\n","params['weights'] = 'EfficientNetB0-CurriculumLearning.h5'\n","params['model_path'] = '/content/weights/' + params['weights']\n","\n","params['data_set'] ='SyntheticDatabase_testingset'\n","params['path_data'] = '/content/data/' + params['data_set'] + '/'\n","params['path_save'] = '/content/results/'+ params['data_set'] +'/'\n","\n","params['isRealVid'] = False\n","params['isSingleFolder'] = False\n","params['imgSave'] = False\n","\n","if not os.path.exists(params['path_save']): os.mkdir(params['path_save'])\n","\n","custom_objects = {'BilinearUpSampling2D': BilinearUpSampling2D}\n","print('Loading model ',params['model_path'] )\n","model = tf.keras.models.load_model(params['model_path'],custom_objects=custom_objects, compile=False)\n","start = time.time()\n","print('Testing...')\n","\n","e = evaluate_colon(model, maxDepth=25, path_data=params['path_data'], params=params)\n","print(\"{:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}\".format('a1', 'a2', 'a3', 'rel', 'rms', 'log_10'))\n","print(\"{:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}\".format(e[0],e[1],e[2],e[3],e[4],e[5]))\n","\n","end = time.time()\n","print('\\nTest time', end-start, 's')\n","\n","print('\\n\\n'+ '-'*100 + '\\nGLOBAL METRICS\\n'+'-'*100)\n","\n","path_vids='./results/' + params['data_set'] + '/'\n","accuracy_list =[]\n","rmse_list = []\n","\n","for vid_name in os.listdir(path_vids):\n","  if vid_name[0] == '.': continue\n","  path_csv = path_vids + vid_name + '/error.csv'\n","  with open(path_csv) as csv_file:\n","    csv_reader = csv.reader(csv_file, delimiter=',')\n","    line_count = 0\n","    for row in csv_reader:\n","      if line_count == 0: accuracy_list.append(float(row[0]))\n","      if line_count == 4: rmse_list.append(float(row[0]))\n","      line_count +=1\n","\n","\n","print('Reported results in Synthetic database (Level 5) with the best configuration of SfSNet:\\n' +\n","      'EfficientNetB0 using Curriculum learning strategy\\n\\n')\n","print('Threshold Accuracy:',np.mean(accuracy_list), '%')\n","print('Root Mean Square Error:',np.mean(rmse_list),'cm' )"],"metadata":{"id":"iQ5SmFL5512X"},"execution_count":null,"outputs":[]}]}